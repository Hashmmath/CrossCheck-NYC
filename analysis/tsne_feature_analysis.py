# #!/usr/bin/env python3
# """
# CrossCheck NYC - t-SNE Feature Analysis
# =======================================
# Creates t-SNE embeddings of tiles based on their OSM feature characteristics.
# Output: JSON file with 2D coordinates for visualization in the dashboard.
# """

# import json
# import numpy as np
# import pandas as pd
# from pathlib import Path
# from typing import Tuple, Dict, List
# import warnings
# warnings.filterwarnings('ignore')

# # Check scikit-learn version for compatibility
# import sklearn
# from sklearn.manifold import TSNE
# from sklearn.preprocessing import OneHotEncoder

# SKLEARN_VERSION = tuple(map(int, sklearn.__version__.split('.')[:2]))
# print(f"scikit-learn version: {sklearn.__version__}")

# # =============================================================================
# # CONFIGURATION
# # =============================================================================
# DATA_DIR = Path("./data")
# METRICS_DIR = DATA_DIR / "results" / "metrics"
# OUTPUT_FILE = METRICS_DIR / "tsne_embeddings.json"

# FEATURE_COLUMNS = [
#     'building_height_category',
#     'tree_category', 
#     'contrast_category',
#     'road_category',
#     'marking_type'
# ]

# FEATURE_DISPLAY_NAMES = {
#     'building_height_category': 'Building Height',
#     'tree_category': 'Tree Proximity',
#     'contrast_category': 'Surface Contrast', 
#     'road_category': 'Road Type',
#     'marking_type': 'Crossing Markings'
# }

# # =============================================================================
# # DATA LOADING
# # =============================================================================
# def load_crosswalk_features() -> pd.DataFrame:
#     """Load the crosswalk features CSV generated by feature_impact_analysis.py"""
#     csv_path = METRICS_DIR / "crosswalk_features.csv"
#     if not csv_path.exists():
#         raise FileNotFoundError(
#             f"Crosswalk features not found at {csv_path}\n"
#             "Run: python analysis/feature_impact_analysis.py first"
#         )
#     return pd.read_csv(csv_path)

# # =============================================================================
# # FEATURE ENCODING
# # =============================================================================
# def encode_features(df: pd.DataFrame) -> Tuple[np.ndarray, pd.DataFrame]:
#     """
#     One-hot encode categorical features for t-SNE.
#     Returns: (encoded_array, original_df_with_tile_info)
#     """
#     # Fill missing values
#     for col in FEATURE_COLUMNS:
#         if col in df.columns:
#             df[col] = df[col].fillna('unknown').astype(str)
#         else:
#             df[col] = 'unknown'
    
#     # Create feature matrix
#     feature_df = df[FEATURE_COLUMNS].copy()
    
#     # One-hot encode - handle different sklearn versions
#     try:
#         # sklearn >= 1.2
#         encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
#     except TypeError:
#         # sklearn < 1.2
#         encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    
#     encoded = encoder.fit_transform(feature_df)
    
#     return encoded, df

# def determine_dominant_feature(row: pd.Series) -> Tuple[str, str]:
#     """
#     Determine which feature most affects this tile.
#     Returns: (feature_name, sub_category)
#     """
#     # Priority order for "affecting" features (features likely to cause detection issues)
#     priority_checks = [
#         ('building_height_category', ['high', 'medium'], 'Building Height'),
#         ('tree_category', ['many', 'moderate', 'few'], 'Tree Proximity'),
#         ('contrast_category', ['low'], 'Surface Contrast'),
#         ('road_category', ['residential', 'other'], 'Road Type'),
#         ('marking_type', ['surface', 'unknown'], 'Crossing Markings'),
#     ]
    
#     for col, affecting_values, display_name in priority_checks:
#         if col in row.index:
#             val = str(row[col]).lower()
#             if val in [v.lower() for v in affecting_values]:
#                 return display_name, str(row[col]).title()
    
#     # Default
#     return 'None', 'No significant factor'

# # =============================================================================
# # t-SNE COMPUTATION
# # =============================================================================
# def compute_tsne(
#     encoded_features: np.ndarray,
#     perplexity: int = 30,
#     max_iterations: int = 1000,
#     random_state: int = 42
# ) -> np.ndarray:
#     """
#     Compute t-SNE embeddings.
    
#     Parameters:
#     - perplexity: Balance between local and global structure (5-50 typical)
#     - max_iterations: Number of iterations for optimization
#     - random_state: For reproducibility
    
#     Returns: 2D array of shape (n_samples, 2)
#     """
#     # Adjust perplexity if needed (must be less than n_samples)
#     n_samples = encoded_features.shape[0]
#     adjusted_perplexity = min(perplexity, max(5, n_samples - 1))
    
#     print(f"  Running t-SNE with perplexity={adjusted_perplexity}, iterations={max_iterations}")
    
#     # Handle different sklearn versions
#     # In sklearn >= 1.2, 'n_iter' was renamed to 'max_iter'
#     try:
#         # Try newer sklearn syntax first (>= 1.2)
#         tsne = TSNE(
#             n_components=2,
#             perplexity=adjusted_perplexity,
#             max_iter=max_iterations,  # New parameter name
#             random_state=random_state,
#             init='pca',
#             learning_rate='auto'
#         )
#     except TypeError:
#         try:
#             # Fall back to older sklearn syntax (< 1.2)
#             tsne = TSNE(
#                 n_components=2,
#                 perplexity=adjusted_perplexity,
#                 n_iter=max_iterations,  # Old parameter name
#                 random_state=random_state,
#                 init='pca',
#                 learning_rate='auto'
#             )
#         except TypeError:
#             # Very old sklearn - minimal parameters
#             tsne = TSNE(
#                 n_components=2,
#                 perplexity=adjusted_perplexity,
#                 random_state=random_state
#             )
    
#     embeddings = tsne.fit_transform(encoded_features)
    
#     return embeddings

# # =============================================================================
# # MAIN ANALYSIS
# # =============================================================================
# def run_tsne_analysis():
#     """Run complete t-SNE analysis and save results."""
    
#     print("\n" + "=" * 70)
#     print("   CROSSCHECK NYC - t-SNE FEATURE ANALYSIS")
#     print("   Dimensionality reduction for feature visualization")
#     print("=" * 70 + "\n")
    
#     # Load data
#     print("Loading crosswalk features...")
#     try:
#         df = load_crosswalk_features()
#     except FileNotFoundError as e:
#         print(f"ERROR: {e}")
#         return None
        
#     print(f"Loaded {len(df)} crosswalk records")
#     print(f"Columns: {list(df.columns)}")
    
#     # Check for location column
#     if 'location_id' not in df.columns:
#         df['location_id'] = 'unknown'
    
#     # Create tile identifier
#     if 'crosswalk_idx' in df.columns:
#         df['tile_id'] = df['crosswalk_idx'].astype(str)
#     else:
#         df['tile_id'] = [f"tile_{i}" for i in range(len(df))]
    
#     # Encode features
#     print("\nEncoding features...")
#     encoded, df = encode_features(df)
#     print(f"Feature matrix shape: {encoded.shape}")
    
#     # Add a small amount of noise to avoid identical rows (t-SNE issue)
#     encoded = encoded + np.random.normal(0, 0.01, encoded.shape)
    
#     # Compute t-SNE per location + overall
#     results = {'overall': [], 'by_location': {}}
    
#     locations = df['location_id'].unique()
#     print(f"\nLocations found: {list(locations)}")
    
#     # Overall t-SNE
#     print("\nComputing overall t-SNE...")
#     if len(df) >= 5:
#         embeddings = compute_tsne(encoded)
        
#         # Add dominant feature info
#         dominant_features = []
#         feature_subcategories = []
#         for idx, row in df.iterrows():
#             dom_feat, sub_cat = determine_dominant_feature(row)
#             dominant_features.append(dom_feat)
#             feature_subcategories.append(sub_cat)
        
#         df['dominant_feature'] = dominant_features
#         df['feature_subcategory'] = feature_subcategories
        
#         # Build overall results
#         overall_points = []
#         for i, (idx, row) in enumerate(df.iterrows()):
#             overall_points.append({
#                 'tile_id': str(row['tile_id']),
#                 'x': float(embeddings[i, 0]),
#                 'y': float(embeddings[i, 1]),
#                 'location': str(row['location_id']),
#                 'detected': bool(row.get('detected', False)),
#                 'dominant_feature': str(row['dominant_feature']),
#                 'feature_subcategory': str(row['feature_subcategory']),
#                 'building_height': str(row.get('building_height_category', 'unknown')),
#                 'tree_proximity': str(row.get('tree_category', 'unknown')),
#                 'surface_contrast': str(row.get('contrast_category', 'unknown')),
#                 'road_type': str(row.get('road_category', 'unknown')),
#                 'marking_type': str(row.get('marking_type', 'unknown'))
#             })
        
#         results['overall'] = overall_points
#         print(f"  Generated {len(overall_points)} overall points")
#     else:
#         print("  Not enough samples for overall t-SNE")
    
#     # Per-location t-SNE
#     for loc in locations:
#         if loc == 'unknown':
#             continue
            
#         loc_mask = df['location_id'] == loc
#         loc_df = df[loc_mask].copy()
#         loc_indices = loc_df.index.tolist()
        
#         if len(loc_df) < 5:
#             print(f"\nSkipping {loc}: only {len(loc_df)} samples (need at least 5)")
#             continue
        
#         print(f"\nComputing t-SNE for {loc} ({len(loc_df)} samples)...")
        
#         loc_encoded = encoded[loc_mask]
        
#         # Adjust perplexity for small datasets
#         loc_perplexity = min(15, max(5, len(loc_df) // 3))
#         loc_embeddings = compute_tsne(loc_encoded, perplexity=loc_perplexity)
        
#         loc_points = []
#         for i, (idx, row) in enumerate(loc_df.iterrows()):
#             loc_points.append({
#                 'tile_id': str(row['tile_id']),
#                 'x': float(loc_embeddings[i, 0]),
#                 'y': float(loc_embeddings[i, 1]),
#                 'detected': bool(row.get('detected', False)),
#                 'dominant_feature': str(row.get('dominant_feature', 'None')),
#                 'feature_subcategory': str(row.get('feature_subcategory', 'Unknown')),
#                 'building_height': str(row.get('building_height_category', 'unknown')),
#                 'tree_proximity': str(row.get('tree_category', 'unknown')),
#                 'surface_contrast': str(row.get('contrast_category', 'unknown')),
#                 'road_type': str(row.get('road_category', 'unknown')),
#                 'marking_type': str(row.get('marking_type', 'unknown'))
#             })
        
#         results['by_location'][loc] = loc_points
#         print(f"  Generated {len(loc_points)} points for {loc}")
    
#     # Save results
#     METRICS_DIR.mkdir(parents=True, exist_ok=True)
#     with open(OUTPUT_FILE, 'w') as f:
#         json.dump(results, f, indent=2)
    
#     print("\n" + "=" * 70)
#     print("RESULTS SUMMARY")
#     print("=" * 70)
#     print(f"\n✓ t-SNE embeddings saved to: {OUTPUT_FILE}")
#     print(f"  - Overall: {len(results['overall'])} points")
#     for loc, points in results['by_location'].items():
#         print(f"  - {loc}: {len(points)} points")
    
#     return results


# if __name__ == "__main__":
#     run_tsne_analysis()

#!/usr/bin/env python3
"""
tsne_feature_analysis.py
Rebuilt: preserves original t-SNE logic + dominant-feature heuristics,
but uses the Tile2Net segmentation filenames (sidebside_x_y_z.png) as tile_id.
Mapping rule (Option A): for each location, take first N CSV rows (original CSV order)
where N = number of segmentation tiles for that location, and assign them to the
segmentation filenames sorted alphabetically.

Outputs:
 - data/results/metrics/tsne_embeddings.json
 - data/results/metrics/tsne_mapping_diagnostics.csv

Compatible with scikit-learn 1.7.2.
"""

import json
import math
import warnings
from pathlib import Path
from typing import Tuple, List, Dict

import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
from sklearn.preprocessing import OneHotEncoder

warnings.filterwarnings("ignore")

# =============================================================================
# CONFIGURATION (kept similar to your original)
# =============================================================================
DATA_DIR = Path("./data")
OUTPUT_DIR = DATA_DIR / "outputs"
RESULTS_DIR = DATA_DIR / "results"
VIZ_DIR = RESULTS_DIR / "visualizations"
METRICS_DIR = RESULTS_DIR / "metrics"
METRICS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_FILE = METRICS_DIR / "tsne_embeddings.json"
DIAG_CSV = METRICS_DIR / "tsne_mapping_diagnostics.csv"

# Feature columns - keep same as original ordering where possible
FEATURE_COLUMNS = [
    'building_height_category',
    'tree_category',
    'contrast_category',
    'road_category',
    'marking_type'
]

FEATURE_DISPLAY_NAMES = {
    'building_height_category': 'Building Height',
    'tree_category': 'Tree Proximity',
    'contrast_category': 'Surface Contrast',
    'road_category': 'Road Type',
    'marking_type': 'Crossing Markings'
}

# Locations (keep same set you used)
LOCATIONS = ["financial_district", "east_village", "bay_ridge", "downtown_brooklyn", "kew_gardens"]

# Candidate paths for crosswalk features CSV (try these in order)
FEATURE_CSV_CANDIDATES = [
    METRICS_DIR / "crosswalk_features.csv",
    RESULTS_DIR / "crosswalk_features.csv",
    DATA_DIR / "crosswalk_features.csv",
    DATA_DIR / "processed" / "crosswalk_features.csv"
]


# =============================================================================
# Helpers: segmentation discovery & Option A mapping
# =============================================================================
def seg_results_path_for(loc: str) -> Path:
    # your pipeline used: data/outputs/<location>/segmentation/<location>/256_19_4/seg_results/
    return OUTPUT_DIR / loc / "segmentation" / loc / "256_19_4" / "seg_results"


def find_sidebside_images(location_id: str) -> List[Path]:
    p = seg_results_path_for(location_id)
    if p.exists():
        return sorted(list(p.glob("sidebside_*.png")))
    # fallback: search segmentation subfolders for seg_results
    fallback = sorted((OUTPUT_DIR / location_id / "segmentation").rglob("seg_results"))
    if fallback:
        return sorted(list(fallback[0].glob("sidebside_*.png")))
    return []


def locate_feature_csv() -> Path:
    for p in FEATURE_CSV_CANDIDATES:
        if p.exists():
            return p
    # try previous default candidate locations if not found
    alt = DATA_DIR / "results" / "metrics" / "crosswalk_features.csv"
    if alt.exists():
        return alt
    raise FileNotFoundError("Could not find crosswalk_features.csv at any known candidate location.")


def map_csv_rows_to_tiles_option_a(df_feat: pd.DataFrame, project_tiles: Dict[str, List[Path]]) -> pd.DataFrame:
    """
    Option A mapping:
    For each location:
      - count N segmentation tiles (project_tiles[loc])
      - take first N rows in df_feat for that location (preserve CSV order)
      - assign those rows to filenames sorted alphabetically
    Returns dataframe with matched_tile_filename and mapping_reason columns (only assigned rows returned)
    """
    assigned_rows = []
    reasons = []
    for loc, files in project_tiles.items():
        n_tiles = len(files)
        if n_tiles == 0:
            continue
        seg_names_sorted = sorted([p.name for p in files])
        # rows for this location, in CSV order
        loc_rows = df_feat[df_feat['location_id'].astype(str) == loc].copy()
        if loc_rows.empty:
            continue
        take_n = min(n_tiles, len(loc_rows))
        loc_rows_take = loc_rows.iloc[:take_n].copy()
        for i, (_, row) in enumerate(loc_rows_take.iterrows()):
            row = row.copy()
            row['matched_tile_filename'] = seg_names_sorted[i]
            row['mapping_reason'] = 'optionA_firstN_assign_by_alpha'
            assigned_rows.append(row)
    if not assigned_rows:
        return pd.DataFrame(columns=list(df_feat.columns) + ['matched_tile_filename', 'mapping_reason'])
    return pd.DataFrame(assigned_rows)


# =============================================================================
# Feature encoding + dominant-feature logic (from your original)
# =============================================================================
def encode_features(df: pd.DataFrame) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    One-hot encode categorical features for t-SNE.
    Returns (encoded_array, original_df_with_tile_info)
    """
    # Fill missing values for expected feature columns
    for col in FEATURE_COLUMNS:
        if col in df.columns:
            df[col] = df[col].fillna('unknown').astype(str)
        else:
            df[col] = 'unknown'
    # Prepare feature matrix
    feature_df = df[FEATURE_COLUMNS].copy()
    # OneHotEncoder using sklearn >= 1.2 param name
    try:
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    except TypeError:
        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(feature_df)
    return encoded, df


def determine_dominant_feature(row: pd.Series) -> Tuple[str, str]:
    """
    Determine which feature most affects this tile (keeps your original priority rules).
    Returns: (feature_name, sub_category)
    """
    priority_checks = [
        ('building_height_category', ['high', 'medium'], 'Building Height'),
        ('tree_category', ['many', 'moderate', 'few'], 'Tree Proximity'),
        ('contrast_category', ['low'], 'Surface Contrast'),
        ('road_category', ['residential', 'other'], 'Road Type'),
        ('marking_type', ['surface', 'unknown'], 'Crossing Markings'),
    ]
    for col, affecting_values, display_name in priority_checks:
        if col in row.index:
            val = str(row[col]).lower()
            if val in [v.lower() for v in affecting_values]:
                return display_name, str(row[col]).title()
    return 'None', 'No significant factor'


# =============================================================================
# t-SNE compute helper (sklearn 1.7.2 compatibility: max_iter)
# =============================================================================
def compute_tsne(encoded_features: np.ndarray, perplexity: int = 30, max_iterations: int = 1000, random_state: int = 42) -> np.ndarray:
    n_samples = encoded_features.shape[0]
    adjusted_perplexity = min(perplexity, max(5, n_samples - 1))
    # Build TSNE with max_iter
    tsne = TSNE(
        n_components=2,
        perplexity=adjusted_perplexity,
        max_iter=max_iterations,
        random_state=random_state,
        init='pca',
        learning_rate='auto'
    )
    embeddings = tsne.fit_transform(encoded_features)
    return embeddings


# =============================================================================
# MAIN ANALYSIS (integrating original logic)
# =============================================================================
def run_tsne_analysis():
    print("\n" + "=" * 70)
    print("   CROSSCHECK NYC - t-SNE FEATURE ANALYSIS (merged)")
    print("   Using Option A mapping (assign CSV rows by location order to seg tiles)")
    print("=" * 70 + "\n")

    # discover project tiles per location
    project_tiles = {}
    for loc in LOCATIONS:
        files = find_sidebside_images(loc)
        project_tiles[loc] = files
        print(f"[INFO] Found {len(files)} segmentation tiles for location '{loc}' at '{seg_results_path_for(loc)}'")

    # load feature CSV
    try:
        csv_path = locate_feature_csv()
    except Exception as e:
        print(f"[FATAL] Could not locate features CSV: {e}")
        return None
    df = pd.read_csv(csv_path)
    print(f"[INFO] Loaded {len(df)} crosswalk records")
    print(f"[INFO] CSV columns: {list(df.columns)}")

    # ensure location_id present
    if 'location_id' not in df.columns:
        df['location_id'] = 'unknown'

    # Map CSV rows to segmentation tiles using Option A
    df_mapped = map_csv_rows_to_tiles_option_a(df, project_tiles)
    if df_mapped.empty:
        print("[FATAL] Option A mapping produced no assigned rows. Exiting.")
        return None

    # At this point df_mapped contains only rows that were assigned to real segmentation files
    # Prepare for encoding: keep the necessary columns, and set tile_id to the filename
    df_mapped['tile_id'] = df_mapped['matched_tile_filename']
    # Keep original columns, but we will use df_mapped for t-SNE
    # Replace missing feature columns
    for col in FEATURE_COLUMNS:
        if col not in df_mapped.columns:
            df_mapped[col] = 'unknown'
        else:
            df_mapped[col] = df_mapped[col].fillna('unknown').astype(str)

    # Add dominant_feature using original function
    dominant_features = []
    feature_subcategories = []
    for idx, row in df_mapped.iterrows():
        dfname, subcat = determine_dominant_feature(row)
        dominant_features.append(dfname)
        feature_subcategories.append(subcat)
    df_mapped['dominant_feature'] = dominant_features
    df_mapped['feature_subcategory'] = feature_subcategories

    # Build results structure similar to original
    results = {'overall': [], 'by_location': {}}

    # Compute overall t-SNE if enough samples (same rule originally: >=5)
    if len(df_mapped) >= 5:
        print("[INFO] Computing overall t-SNE...")
        encoded, df_encoded = encode_features(df_mapped)
        # add noise
        encoded = encoded + np.random.normal(0, 0.01, encoded.shape)
        embeddings = compute_tsne(encoded, max_iterations=1000)

        # Fill df_encoded with embeddings
        # df_encoded['x'] = embeddings[:, 0]
        # df_encoded['y'] = embeddings[:, 1]

        # Compose overall points preserving original fields for compatibility
        for i, (idx, row) in enumerate(df_encoded.iterrows()):
            results['overall'].append({
                'tile_id': str(row['tile_id']),
                'x': float(embeddings[i, 0]),
                'y': float(embeddings[i, 1]),
                'location': str(row.get('location_id', 'unknown')),
                'detected': bool(row.get('detected', False)) if 'detected' in row else False,
                'dominant_feature': str(row.get('dominant_feature', 'None')),
                'feature_subcategory': str(row.get('feature_subcategory', 'Unknown')),
                'building_height': str(row.get('building_height_category', 'unknown')),
                'tree_proximity': str(row.get('tree_category', 'unknown')),
                'surface_contrast': str(row.get('contrast_category', 'unknown')),
                'road_type': str(row.get('road_category', 'unknown')),
                'marking_type': str(row.get('marking_type', 'unknown'))
            })
        print(f"  Generated {len(results['overall'])} overall points")
    else:
        print("[INFO] Not enough samples for overall t-SNE (need >=5)")

    # Per-location t-SNE (same behavior as original)
    for loc in df_mapped['location_id'].unique():
        if loc == 'unknown' or not str(loc).strip():
            continue
        loc_df = df_mapped[df_mapped['location_id'] == loc].copy()
        n_loc = len(loc_df)
        if n_loc < 5:
            print(f"[INFO] Skipping {loc}: only {n_loc} samples (need at least 5) for location-level t-SNE")
            # still add entries to results['by_location'] as empty or with available points?
            # Original did skip; we follow that behavior (no by_location entry or empty list)
            continue

        print(f"[INFO] Computing t-SNE for {loc} ({n_loc} samples)...")
        # Encode only loc_df features
        enc_loc, loc_encoded_df = encode_features(loc_df)
        enc_loc = enc_loc + np.random.normal(0, 0.01, enc_loc.shape)
        loc_embeddings = compute_tsne(enc_loc, max_iterations=1000)

        # loc_encoded_df['x'] = loc_embeddings[:, 0]
        # loc_encoded_df['y'] = loc_embeddings[:, 1]

        points = []
        for i, (idx, row) in enumerate(loc_encoded_df.iterrows()):
            points.append({
                'tile_id': str(row['tile_id']),
                # 'x': float(row['x']),
                # 'y': float(row['y']),
                'x': float(loc_embeddings[i, 0]),
                'y': float(loc_embeddings[i, 1]),
                'detected': bool(row.get('detected', False)) if 'detected' in row else False,
                'dominant_feature': str(row.get('dominant_feature', 'None')),
                'feature_subcategory': str(row.get('feature_subcategory', 'Unknown')),
                'building_height': str(row.get('building_height_category', 'unknown')),
                'tree_proximity': str(row.get('tree_category', 'unknown')),
                'surface_contrast': str(row.get('contrast_category', 'unknown')),
                'road_type': str(row.get('road_category', 'unknown')),
                'marking_type': str(row.get('marking_type', 'unknown'))
            })
        results['by_location'][loc] = points
        print(f"  Generated {len(points)} points for {loc}")

    # Save JSON
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(results, f, indent=2)

    # Save diagnostics: include mapping data
    diag = df_mapped.copy()
    # Keep a few relevant columns for debugging
    diag_cols = [c for c in diag.columns if c in ['location_id', 'matched_tile_filename', 'mapping_reason', 'tile_id'] + FEATURE_COLUMNS + ['dominant_feature', 'feature_subcategory']]
    diag_out = diag[diag_cols] if diag_cols else diag
    diag_out.to_csv(DIAG_CSV, index=False)

    print("\n" + "=" * 70)
    print("RESULTS SUMMARY")
    print("=" * 70)
    print(f"\n✓ t-SNE embeddings saved to: {OUTPUT_FILE}")
    for loc, pts in results['by_location'].items():
        print(f"  - {loc}: {len(pts)} points")
    return results


if __name__ == "__main__":
    run_tsne_analysis()
