#!/usr/bin/env python3
"""
CrossCheck NYC - t-SNE Feature Analysis
=======================================
Creates t-SNE embeddings of tiles based on their OSM feature characteristics.
Output: JSON file with 2D coordinates for visualization in the dashboard.
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')

# Check scikit-learn version for compatibility
import sklearn
from sklearn.manifold import TSNE
from sklearn.preprocessing import OneHotEncoder

SKLEARN_VERSION = tuple(map(int, sklearn.__version__.split('.')[:2]))
print(f"scikit-learn version: {sklearn.__version__}")

# =============================================================================
# CONFIGURATION
# =============================================================================
DATA_DIR = Path("./data")
METRICS_DIR = DATA_DIR / "results" / "metrics"
OUTPUT_FILE = METRICS_DIR / "tsne_wholeloc_embeddings.json"

FEATURE_COLUMNS = [
    'building_height_category',
    'tree_category', 
    'contrast_category',
    'road_category',
    'marking_type'
]

FEATURE_DISPLAY_NAMES = {
    'building_height_category': 'Building Height',
    'tree_category': 'Tree Proximity',
    'contrast_category': 'Surface Contrast', 
    'road_category': 'Road Type',
    'marking_type': 'Crossing Markings'
}

# =============================================================================
# DATA LOADING
# =============================================================================
def load_crosswalk_features() -> pd.DataFrame:
    """Load the crosswalk features CSV generated by feature_impact_analysis.py"""
    csv_path = METRICS_DIR / "crosswalk_features.csv"
    if not csv_path.exists():
        raise FileNotFoundError(
            f"Crosswalk features not found at {csv_path}\n"
            "Run: python analysis/feature_impact_analysis.py first"
        )
    return pd.read_csv(csv_path)

# =============================================================================
# FEATURE ENCODING
# =============================================================================
def encode_features(df: pd.DataFrame) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    One-hot encode categorical features for t-SNE.
    Returns: (encoded_array, original_df_with_tile_info)
    """
    # Fill missing values
    for col in FEATURE_COLUMNS:
        if col in df.columns:
            df[col] = df[col].fillna('unknown').astype(str)
        else:
            df[col] = 'unknown'
    
    # Create feature matrix
    feature_df = df[FEATURE_COLUMNS].copy()
    
    # One-hot encode - handle different sklearn versions
    try:
        # sklearn >= 1.2
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    except TypeError:
        # sklearn < 1.2
        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    
    encoded = encoder.fit_transform(feature_df)
    
    return encoded, df

def determine_dominant_feature(row: pd.Series) -> Tuple[str, str]:
    """
    Determine which feature most affects this tile.
    Returns: (feature_name, sub_category)
    """
    # Priority order for "affecting" features (features likely to cause detection issues)
    priority_checks = [
        ('building_height_category', ['high', 'medium'], 'Building Height'),
        ('tree_category', ['many', 'moderate', 'few'], 'Tree Proximity'),
        ('contrast_category', ['low'], 'Surface Contrast'),
        ('road_category', ['residential', 'other'], 'Road Type'),
        ('marking_type', ['surface', 'unknown'], 'Crossing Markings'),
    ]
    
    for col, affecting_values, display_name in priority_checks:
        if col in row.index:
            val = str(row[col]).lower()
            if val in [v.lower() for v in affecting_values]:
                return display_name, str(row[col]).title()
    
    # Default
    return 'None', 'No significant factor'

# =============================================================================
# t-SNE COMPUTATION
# =============================================================================
def compute_tsne(
    encoded_features: np.ndarray,
    perplexity: int = 30,
    max_iterations: int = 1000,
    random_state: int = 42
) -> np.ndarray:
    """
    Compute t-SNE embeddings.
    
    Parameters:
    - perplexity: Balance between local and global structure (5-50 typical)
    - max_iterations: Number of iterations for optimization
    - random_state: For reproducibility
    
    Returns: 2D array of shape (n_samples, 2)
    """
    # Adjust perplexity if needed (must be less than n_samples)
    n_samples = encoded_features.shape[0]
    adjusted_perplexity = min(perplexity, max(5, n_samples - 1))
    
    print(f"  Running t-SNE with perplexity={adjusted_perplexity}, iterations={max_iterations}")
    
    # Handle different sklearn versions
    # In sklearn >= 1.2, 'n_iter' was renamed to 'max_iter'
    try:
        # Try newer sklearn syntax first (>= 1.2)
        tsne = TSNE(
            n_components=2,
            perplexity=adjusted_perplexity,
            max_iter=max_iterations,  # New parameter name
            random_state=random_state,
            init='pca',
            learning_rate='auto'
        )
    except TypeError:
        try:
            # Fall back to older sklearn syntax (< 1.2)
            tsne = TSNE(
                n_components=2,
                perplexity=adjusted_perplexity,
                n_iter=max_iterations,  # Old parameter name
                random_state=random_state,
                init='pca',
                learning_rate='auto'
            )
        except TypeError:
            # Very old sklearn - minimal parameters
            tsne = TSNE(
                n_components=2,
                perplexity=adjusted_perplexity,
                random_state=random_state
            )
    
    embeddings = tsne.fit_transform(encoded_features)
    
    return embeddings

# =============================================================================
# MAIN ANALYSIS
# =============================================================================
def run_tsne_analysis():
    """Run complete t-SNE analysis and save results."""
    
    print("\n" + "=" * 70)
    print("   CROSSCHECK NYC - t-SNE FEATURE ANALYSIS")
    print("   Dimensionality reduction for feature visualization")
    print("=" * 70 + "\n")
    
    # Load data
    print("Loading crosswalk features...")
    try:
        df = load_crosswalk_features()
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return None
        
    print(f"Loaded {len(df)} crosswalk records")
    print(f"Columns: {list(df.columns)}")
    
    # Check for location column
    if 'location_id' not in df.columns:
        df['location_id'] = 'unknown'
    
    # Create tile identifier
    if 'crosswalk_idx' in df.columns:
        df['tile_id'] = df['crosswalk_idx'].astype(str)
    else:
        df['tile_id'] = [f"tile_{i}" for i in range(len(df))]
    
    # Encode features
    print("\nEncoding features...")
    encoded, df = encode_features(df)
    print(f"Feature matrix shape: {encoded.shape}")
    
    # Add a small amount of noise to avoid identical rows (t-SNE issue)
    encoded = encoded + np.random.normal(0, 0.01, encoded.shape)
    
    # Compute t-SNE per location + overall
    results = {'overall': [], 'by_location': {}}
    
    locations = df['location_id'].unique()
    print(f"\nLocations found: {list(locations)}")
    
    # Overall t-SNE
    print("\nComputing overall t-SNE...")
    if len(df) >= 5:
        embeddings = compute_tsne(encoded)
        
        # Add dominant feature info
        dominant_features = []
        feature_subcategories = []
        for idx, row in df.iterrows():
            dom_feat, sub_cat = determine_dominant_feature(row)
            dominant_features.append(dom_feat)
            feature_subcategories.append(sub_cat)
        
        df['dominant_feature'] = dominant_features
        df['feature_subcategory'] = feature_subcategories
        
        # Build overall results
        overall_points = []
        for i, (idx, row) in enumerate(df.iterrows()):
            overall_points.append({
                'tile_id': str(row['tile_id']),
                'x': float(embeddings[i, 0]),
                'y': float(embeddings[i, 1]),
                'location': str(row['location_id']),
                'detected': bool(row.get('detected', False)),
                'dominant_feature': str(row['dominant_feature']),
                'feature_subcategory': str(row['feature_subcategory']),
                'building_height': str(row.get('building_height_category', 'unknown')),
                'tree_proximity': str(row.get('tree_category', 'unknown')),
                'surface_contrast': str(row.get('contrast_category', 'unknown')),
                'road_type': str(row.get('road_category', 'unknown')),
                'marking_type': str(row.get('marking_type', 'unknown'))
            })
        
        results['overall'] = overall_points
        print(f"  Generated {len(overall_points)} overall points")
    else:
        print("  Not enough samples for overall t-SNE")
    
    # Per-location t-SNE
    for loc in locations:
        if loc == 'unknown':
            continue
            
        loc_mask = df['location_id'] == loc
        loc_df = df[loc_mask].copy()
        loc_indices = loc_df.index.tolist()
        
        if len(loc_df) < 5:
            print(f"\nSkipping {loc}: only {len(loc_df)} samples (need at least 5)")
            continue
        
        print(f"\nComputing t-SNE for {loc} ({len(loc_df)} samples)...")
        
        loc_encoded = encoded[loc_mask]
        
        # Adjust perplexity for small datasets
        loc_perplexity = min(15, max(5, len(loc_df) // 3))
        loc_embeddings = compute_tsne(loc_encoded, perplexity=loc_perplexity)
        
        loc_points = []
        for i, (idx, row) in enumerate(loc_df.iterrows()):
            loc_points.append({
                'tile_id': str(row['tile_id']),
                'x': float(loc_embeddings[i, 0]),
                'y': float(loc_embeddings[i, 1]),
                'detected': bool(row.get('detected', False)),
                'dominant_feature': str(row.get('dominant_feature', 'None')),
                'feature_subcategory': str(row.get('feature_subcategory', 'Unknown')),
                'building_height': str(row.get('building_height_category', 'unknown')),
                'tree_proximity': str(row.get('tree_category', 'unknown')),
                'surface_contrast': str(row.get('contrast_category', 'unknown')),
                'road_type': str(row.get('road_category', 'unknown')),
                'marking_type': str(row.get('marking_type', 'unknown'))
            })
        
        results['by_location'][loc] = loc_points
        print(f"  Generated {len(loc_points)} points for {loc}")
    
    # Save results
    METRICS_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\n" + "=" * 70)
    print("RESULTS SUMMARY")
    print("=" * 70)
    print(f"\nâœ“ t-SNE embeddings saved to: {OUTPUT_FILE}")
    print(f"  - Overall: {len(results['overall'])} points")
    for loc, points in results['by_location'].items():
        print(f"  - {loc}: {len(points)} points")
    
    return results


if __name__ == "__main__":
    run_tsne_analysis()